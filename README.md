# ğŸ’¬ğŸ§  Twitter Emotion Recognition using Deep Learning & Transformers
### Deep Learning & Applications (UEC642) â€” Final Project

A complete end-to-end system for emotion classification in tweets using multiple deep learning approaches, ranging from classical LSTMs to modern Transformer-based models.  
This project evaluates three separate models and compares their performance:

- Original LSTM (Random Embeddings)  
- Improved LSTM (GloVe Pre-trained Embeddings)  
- RoBERTa Transformer (State-of-the-Art)  

This repository contains the full code, evaluation pipeline, comparison metrics, visualizations, and model benchmarking.

---

## ğŸ‘¨â€ğŸ“ Submitted To
**Dr. Gaganpreet Kaur**

## ğŸ‘¨â€ğŸ“ Submitted By
- **Kanav Kukreja â€” 102215145**  
- **Priyanshu â€” 102215164**  
- **Vinaayak Kumar Puri â€” 102215165**  
- **Punya Arora â€” 102215186**

---

# ğŸ“Œ Table of Contents
- [Project Overview](#-project-overview)
- [Dataset](#-dataset)
- [Emotion Classes](#-emotion-classes)
- [Model Architectures](#-model-architectures)
- [Training Setup](#-training-setup)
- [Results Summary](#-results-summary)
- [Full Model Comparison](#-full-model-comparison)
- [Visualizations](#-visualizations)
- [Use-Case Recommendations](#-use-case-recommendations)
- [Project Structure](#-project-structure)
- [How to Run](#-how-to-run)
- [Project Report](#-project-report)

---

# ğŸŒŸ Project Overview
The goal of this project is to classify English tweets into one of six emotion categories using machine learning and deep learning.

The project includes:

âœ”ï¸ Complete preprocessing pipeline  
âœ”ï¸ Tokenization & padding  
âœ”ï¸ LSTM models (baseline & improved)  
âœ”ï¸ GloVe 100-dimensional embeddings  
âœ”ï¸ RoBERTa transformer classifier  
âœ”ï¸ Confusion matrices  
âœ”ï¸ F1, Precision, Recall benchmarking  
âœ”ï¸ Model agreement analysis  
âœ”ï¸ Comprehensive visualizations  

The entire implementation is contained inside:  
ğŸ“„ **tweet emotion recognition.py**

---

# ğŸ“Š Dataset
The project uses the HuggingFace â€œemotionâ€ dataset:

| Split | Samples |
|---------|----------|
| Training | 16,000 |
| Validation | 2,000 |
| Test | 2,000 |

---

# ğŸ­ Emotion Classes
The dataset maps emotions using:
{0: sadness, 1: joy, 2: love, 3: anger, 4: fear, 5: surprise}

---

# ğŸ§  Model Architectures

## 1ï¸âƒ£ Original LSTM Model (Baseline)
- Random embedding layer (size 16)  
- Bi-directional LSTM (20 units Ã— 2 layers)  
- Softmax output for 6 emotions  
**Purpose:** Fast baseline for comparison.

---

## 2ï¸âƒ£ Improved LSTM Model (GloVe)
- GloVe 100-dimensional pretrained embeddings  
- Bi-LSTM with 64 â†’ 32 hidden units  
- Dropout regularization  
- Dense classifier  
**Purpose:** Improved semantic understanding and F1 score.

---

## 3ï¸âƒ£ RoBERTa Transformer Model
Using model:  
**j-hartmann/emotion-english-distilroberta-base**

- Contextual transformer embeddings  
- SOTA emotion classification  
- Evaluated on full test set using report metrics  
**Purpose:** Highest representational capability and contextual understanding.

---

# âš™ï¸ Training Setup

| Parameter | Value |
|----------|--------|
| Max sequence length | 50 tokens |
| Vocabulary size | 10,000 words |
| Optimizer | Adam |
| Loss | Sparse Crossentropy |
| Callbacks | EarlyStopping, ReduceLROnPlateau |
| GloVe Embeddings | glove.twitter.27B.100d |

---

# ğŸ† Results Summary

## ğŸ“Œ Original LSTM (Random Embeddings)
- **Accuracy:** 87.75%  
- **Weighted F1 Score:** 0.8780  
- **Precision:** 0.8790  
- **Recall:** 0.8775  

---

## ğŸ“Œ Improved LSTM (GloVe Embeddings)
- **Accuracy:** 92.70%  
- **Weighted F1 Score:** 0.9264  
- **Precision:** 0.9293  
- **Recall:** 0.9270  
- **Improvement vs Baseline:**  
  - Accuracy: +4.95%  
  - F1 Score: +0.0484 (4.84%)  

---

## ğŸ“Œ RoBERTa Transformer (SOTA)
- **Accuracy:** 88.89%  
- **Weighted F1 Score:** 0.8672  
- **Precision:** 0.8476  
- **Recall:** 0.8889  

---

# ğŸ“ˆ Full Model Comparison

| Model | Accuracy | Weighted F1 | Precision | Recall |
|--------|-----------|--------------|------------|---------|
| **Original LSTM** | 87.75% | 0.8780 | 0.8790 | 0.8775 |
| **Improved LSTM** | 92.70% | 0.9264 | 0.9293 | 0.9270 |
| **RoBERTa** | 88.89% | 0.8672 | 0.8476 | 0.8889 |

---

# ğŸ¨ Visualizations

The project automatically generates:

- ğŸ“Œ Accuracy Comparison (all 3 models)  
- ğŸ“Œ F1 Score Comparison  
- ğŸ“Œ Confusion Matrices  
- ğŸ“Œ Model Agreement Pie Chart  
- ğŸ“Œ Emotion Distribution Heatmap  
- ğŸ“Œ Confidence Plots  

All visualizations appear directly during script execution.

---

# ğŸ” Use-Case Recommendations

### âœ”ï¸ Use Original LSTM When:
- Low-latency is required  
- Edge devices  
- Quick inference  

### âœ”ï¸ Use Improved LSTM (GloVe) When:
- Balanced speed + accuracy needed  
- Medium-scale apps  
- Highest statistical performance  

### âœ”ï¸ Use RoBERTa When:
- Maximum contextual accuracy needed  
- Transformer-level linguistic understanding  
- Server-based inference pipelines  

---

# ğŸ“ Project Structure
Twitter-Emotion-Recognition/
â”‚â”€â”€ tweet emotion recognition.py # Full implementation
â””â”€â”€ README.md # Documentation

---

# â–¶ï¸ How to Run

### 1. Install dependencies
pip install numpy pandas tensorflow matplotlib seaborn sklearn datasets transformers torch

### 2. Run the Python file
python "tweet emotion recognition.py"

### 3. View Outputs  
The script prints:

- Training logs  
- Validation metrics  
- Test evaluation  
- Confusion matrices  
- Model comparison tables  
- Visualizations  

---

# ğŸ“„ Project Report  
Download and view the complete project report here:  
ğŸ‘‰ **[Download & View Report](https://docs.google.com/document/d/1uaZi6xD16Hv_5GaH6mTZF2B1FznTtUIq-C40tKcn9Zg/edit?usp=sharing)**

---

# ğŸ‰ Final Note
This project demonstrates a full progression from a classical LSTM model to modern transformer-based deep learning for emotion analysis, including benchmarking, visualizations, embeddings, comparisons, and insights â€” making it a complete academic and practical submission.

If you liked this project, â­ star the repository!
