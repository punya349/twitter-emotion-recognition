# ğŸ’¬ğŸ§  Twitter Emotion Recognition using Deep Learning & Transformers  
### **Deep Learning & Applications (UEC642) â€” Final Project**

A complete end-to-end system for emotion classification in tweets using multiple deep learning approaches, ranging from classical LSTMs to modern Transformer-based models.  
This project evaluates **three separate models** and compares their performance:

1. **Original LSTM (Random Embeddings)**  
2. **Improved LSTM (GloVe Pre-trained Embeddings)**  
3. **RoBERTa Transformer (State-of-the-Art)**  

This repository contains the full code, evaluation pipeline, comparison metrics, visualizations, and model benchmarking.

---

# ğŸ‘¨â€ğŸ“ Submitted To  
**Dr. Gaganpreet Kaur**

# ğŸ‘¨â€ğŸ“ Submitted By  
- **Kanav Kukreja â€” 102215145**  
- **Priyanshu â€” 102215164**  
- **Vinaayak Kumar Puri â€” 102215165**  
- **Punya Arora â€” 102215186**

---

# ğŸ“Œ Table of Contents
- [Project Overview](#project-overview)  
- [Dataset](#dataset)  
- [Emotion Classes](#emotion-classes)  
- [Model Architectures](#model-architectures)  
- [Training Setup](#training-setup)  
- [Results Summary](#results-summary)  
- [Full Model Comparison](#full-model-comparison)  
- [Visualizations](#visualizations)  
- [Use-Case Recommendations](#use-case-recommendations)  
- [Project Structure](#project-structure)  
- [How to Run](#how-to-run)

---

# ğŸŒŸ Project Overview
The goal of this project is to classify English tweets into one of **six emotion categories** using machine learning and deep learning.

The project includes:

- âœ”ï¸ Complete preprocessing pipeline  
- âœ”ï¸ Tokenization & padding  
- âœ”ï¸ LSTM models (baseline & improved)  
- âœ”ï¸ GloVe 100-dimensional embeddings  
- âœ”ï¸ RoBERTa transformer classifier  
- âœ”ï¸ Confusion matrices  
- âœ”ï¸ F1, Precision, Recall benchmarking  
- âœ”ï¸ Model agreement analysis  
- âœ”ï¸ Comprehensive visualizations  

The entire implementation is contained inside:  
ğŸ“„ **tweet emotion recognition.py** :contentReference[oaicite:0]{index=0}

---

# ğŸ“Š Dataset
The project uses the **HuggingFace â€œemotionâ€ dataset**:

| Split | Samples |
|-------|----------|
| Training | 16,000 |
| Validation | 2,000 |
| Test | 2,000 |

---

# ğŸ­ Emotion Classes
The dataset maps emotions using:
{0: sadness,
1: joy,
2: love,
3: anger,
4: fear,
5: surprise}


---

# ğŸ§  Model Architectures

## **1ï¸âƒ£ Original LSTM Model (Baseline)**
- Random embedding layer (size 16)  
- Bi-directional LSTM (20 units Ã— 2 layers)  
- Softmax output for 6 emotions  

**Purpose:** Fast baseline for comparison.

---

## **2ï¸âƒ£ Improved LSTM Model (GloVe)**
- GloVe 100-dimensional pretrained embeddings  
- Bi-LSTM with larger hidden units (64 â†’ 32)  
- Dropout for regularization  
- Additional Dense layer  

**Purpose:** Increase semantic understanding and F1 score.

---

## **3ï¸âƒ£ RoBERTa Transformer Model**
Using model:  
`j-hartmann/emotion-english-distilroberta-base`

- Contextual transformer embeddings  
- SOTA emotion classification  
- Evaluated on 100 test samples due to transformer inference cost  

**Purpose:** Highest accuracy and contextual understanding.

---

# âš™ï¸ Training Setup

| Parameter | Value |
|-----------|--------|
| Max sequence length | 50 tokens |
| Vocabulary size | 10,000 words |
| Optimizer | Adam |
| Loss | Sparse Crossentropy |
| Callbacks | EarlyStopping, ReduceLROnPlateau |
| GloVe Embeddings | glove.twitter.27B.100d |

---

# ğŸ† Results Summary

## **ğŸ“Œ Original LSTM (Random Embeddings)**
- **Accuracy:** Printed during evaluation  
- **F1 Score:** Printed  
- **Recall:** Printed  
- **Precision:** Printed  

---

## **ğŸ“Œ Improved LSTM (GloVe Embeddings)**
- Higher accuracy  
- Higher F1 score  
- Better generalization  
- Clear improvement over baseline  

---

## **ğŸ“Œ RoBERTa Transformer (SOTA)**
Evaluated on **100 tweets**.  
Results printed in the file include:

- Accuracy  
- F1 Score  
- Precision  
- Recall  
- Classification report  
- Confusion matrix  
- Per-emotion performance  

---

# ğŸ“ˆ Full Model Comparison (Printed in Output)

The code prints a table summarizing:

| Model | Accuracy | F1 Score | Precision | Recall |
|--------|-----------|-----------|------------|----------|
| Original LSTM | Values printed | Printed | Printed | Printed |
| Improved LSTM | Values printed | Printed | Printed | Printed |
| RoBERTa | Values printed | Printed | Printed | Printed |

It also prints improvements such as:

- F1 improvement from LSTM â†’ GloVe  
- F1 improvement from LSTM â†’ RoBERTa  
- Accuracy increases  

---

# ğŸ¨ Visualizations

The project automatically generates visualizations:

### ğŸ“Œ Accuracy Comparison  
- Original vs Improved vs RoBERTa  

### ğŸ“Œ F1 Score Comparison  
- Highlights F1 as the **primary metric**

### ğŸ“Œ Confusion Matrices  
- For original  
- For improved  
- For RoBERTa  

### ğŸ“Œ Model Agreement Pie Chart  
Shows how often all 3 models agree, 2 agree, or none agree.

### ğŸ“Œ Emotion Distribution Heatmap  
Across all 3 models.

### ğŸ“Œ Confidence Plots  
For each test tweet across all 3 models.

All visualizations are created using Matplotlib and displayed directly during execution.

---

# ğŸ” Use-Case Recommendations (Included in Code Output)

### âœ”ï¸ Use Original LSTM When:
- Low-latency is required  
- Edge devices  
- Quick inference  

### âœ”ï¸ Use Improved LSTM (GloVe) When:
- Balanced speed + accuracy needed  
- Medium-scale apps  
- F1 score is priority  

### âœ”ï¸ Use RoBERTa When:
- Maximum accuracy needed  
- Batch inference  
- Server-side deployments  

---

# ğŸ“ Project Structure
Twitter-Emotion-Recognition/
â”‚â”€â”€ tweet emotion recognition.py # Full end-to-end implementation
â”‚â”€â”€ README.md # Documentation
â”‚â”€â”€ results/ # (Optional) Save plots manually if needed
â””â”€â”€ models/ # (Optional) Save models if exporting

---

# â–¶ï¸ How to Run

### **1. Install dependencies**
pip install numpy pandas tensorflow matplotlib seaborn sklearn datasets transformers torch

### **2. Run the Python file**
python "tweet emotion recognition.py"

### **3. View Outputs**
This file contains:

- Training logs  
- Validation metrics  
- Test evaluation  
- Confusion matrices  
- Model comparison tables  
- Visualizations  

Everything runs automatically â€” no manual steps required.

---

# ğŸ‰ Final Note  
This project demonstrates a full progression from a classical LSTM model to modern transformer-based deep learning for emotion analysis.  
It includes benchmarking, visualizations, comparisons, embeddings, and insights â€” making it a complete academic + practical submission.

If you liked this project, â­ star the repository!

